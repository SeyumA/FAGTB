{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FAGTB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOFxbOmG+oQLIZRYe0NLRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent-grari/FAGTB/blob/master/FAGTB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmKbXdtm_BvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "43127047-e518-458d-9eaa-fbff5f2605ca"
      },
      "source": [
        "!pip3 install fairness\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairness in /usr/local/lib/python3.6/dist-packages (0.1.8)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from fairness) (0.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (2.8.1)\n",
            "Requirement already satisfied: BlackBoxAuditing>=0.1.26ggplot in /usr/local/lib/python3.6/dist-packages (from fairness) (0.1.54)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from fairness) (2.4.7)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from fairness) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from fairness) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fairness) (2018.9)\n",
            "Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->fairness) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (1.18.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (2.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.1->fairness) (0.15.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8t15SobVx31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "164de12f-cc48-4340-96fc-e87133620564"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://grari.fr/Fairness/ProcessedData.py', '/usr/local/lib/python3.6/dist-packages/fairness/data/objects/ProcessedData.py')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/usr/local/lib/python3.6/dist-packages/fairness/data/objects/ProcessedData.py',\n",
              " <http.client.HTTPMessage at 0x7fb4b7697b70>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNxgkEwlE0-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fire\n",
        "import os\n",
        "import statistics\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from fairness import results\n",
        "from fairness.data.objects.list import DATASETS, get_dataset_names\n",
        "from fairness.data.objects.ProcessedData import ProcessedData\n",
        "from fairness.benchmark import run_alg\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "dataset = DATASETS[1] # Adult data set\n",
        "\n",
        "from fairness.algorithms.zafar.ZafarAlgorithm import ZafarAlgorithmBaseline, ZafarAlgorithmAccuracy, ZafarAlgorithmFairness\n",
        "from fairness.algorithms.kamishima.KamishimaAlgorithm import KamishimaAlgorithm\n",
        "from fairness.algorithms.kamishima.CaldersAlgorithm import CaldersAlgorithm\n",
        "from fairness.algorithms.feldman.FeldmanAlgorithm import FeldmanAlgorithm\n",
        "from fairness.algorithms.baseline.SVM import SVM\n",
        "from fairness.algorithms.baseline.DecisionTree import DecisionTree\n",
        "from fairness.algorithms.baseline.GaussianNB import GaussianNB\n",
        "from fairness.algorithms.baseline.LogisticRegression import LogisticRegression\n",
        "from fairness.algorithms.ParamGridSearch import ParamGridSearch\n",
        "from fairness.algorithms.Ben.SDBSVM import SDBSVM\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def p_rule(y_pred, z_values, threshold=0.5):\n",
        "    \n",
        "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
        "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
        "    odds = y_z_1.mean() / y_z_0.mean()\n",
        "    return np.min([odds, 1/odds]) * 100\n",
        "\n",
        "\n",
        "class Sigmoid():\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return self.__call__(x) * (1 - self.__call__(x))\n",
        "    \n",
        "def DispFNR(y_pred, y, z_values, threshold=0.5):\n",
        "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
        "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
        "    result=abs(ypred_z_1[(y==1) & (z_values==0)].mean()-ypred_z_1[(y==1) & (z_values==1)].mean())\n",
        "    return result\n",
        "def DispFPR(y_pred, y, z_values, threshold=0.5):\n",
        "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
        "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
        "    result=abs(ypred_z_1[(y==0) & (z_values==0)].mean()-ypred_z_1[(y==0) & (z_values==1)].mean())\n",
        "    return result\n",
        "\n",
        "\n",
        "def DI(y_pred, z_values, threshold=0.5):\n",
        "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
        "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
        "    odds = abs(y_z_1.mean() - y_z_0.mean())\n",
        "    return odds\n",
        "\n",
        "def display_results(y_pred, y, sensitive):\n",
        "    y_pred2 = (y_pred>0.5).astype(int)\n",
        "    accuracy = accuracy_score(y, np.squeeze(y_pred2))\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"PRULE : \", p_rule(y_pred,sensitive))\n",
        "    print(\"DI : \", DI(y_pred, sensitive))\n",
        "    print(\"DispFPR : \", DispFPR(y_pred2, y, sensitive))\n",
        "    print(\"DispFNR : \", DispFNR(y_pred2, y, sensitive))\n",
        "    return {'Accuracy': accuracy, 'PRULE': p_rule(y_pred,sensitive), 'DispFPR': DispFPR(y_pred2, y, sensitive)\n",
        "            ,'DispFNR': DispFNR(y_pred2, y, sensitive)}\n",
        "\n",
        "def DATA_TRAIN_TEST(num,sens,y,columns_delete):\n",
        "    dataset = DATASETS[num] # Adult data set\n",
        "    all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
        "    ProcessedData(dataset)\n",
        "    processed_dataset = ProcessedData(dataset)\n",
        "    train_test_splits = processed_dataset.create_train_test_splits(1)\n",
        "    train_test_splits.keys()\n",
        "    train, test = train_test_splits['numerical-binsensitive'][0]\n",
        "    X_train = train\n",
        "    X_test = test\n",
        "    sensitive =  train[sens].values\n",
        "    sensitivet =  test[sens].values\n",
        "    y_train = train[y]\n",
        "    y_test = test[y]\n",
        "    \n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    s=X_train[sens]\n",
        "    st=X_test[sens]\n",
        "    t=X_train[y]\n",
        "    tt=X_test[y]\n",
        "    scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
        "    X_train = X_train.pipe(scale_df, scaler)\n",
        "    X_test = X_test.pipe(scale_df, scaler)\n",
        "    X_train= X_train.drop([sens,y], axis=1)\n",
        "    X_train[sens] = s\n",
        "    X_train[y] = t\n",
        "    X_test= X_test.drop([sens,y], axis=1)\n",
        "    X_test[sens] = st\n",
        "    X_test[y] = tt\n",
        "\n",
        "    X_train = X_train.drop(columns_delete,1)\n",
        "    X_test = X_test.drop(columns_delete,1)\n",
        "    return X_train, X_test, y_train, y_test, sensitive, sensitivet\n",
        "\n",
        "\n",
        "def DATA_TRAIN_TEST2(num,sens,y,columns_delete,scaling):\n",
        "    dataset = DATASETS[num] # Adult data set\n",
        "    all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
        "    ProcessedData(dataset)\n",
        "    processed_dataset = ProcessedData(dataset)\n",
        "    train_test_splits = processed_dataset.create_train_test_splits(1)\n",
        "    train_test_splits.keys()\n",
        "    train, test = train_test_splits['numerical-binsensitive'][0]\n",
        "    #all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
        "    #for supported_tag in algorithm.get_supported_data_types():\n",
        "    supported_tag = 'numerical-binsensitive'\n",
        "    privileged_vals = dataset.get_privileged_class_names_with_joint(supported_tag)\n",
        "    positive_val = dataset.get_positive_class_val(supported_tag)\n",
        "    sensitive = sens\n",
        "    print(\"===================================\")\n",
        "    print(\"supported tag: \" + supported_tag)\n",
        "    print(\"sensitive attribute: \" + sensitive)\n",
        "    print(\"===================================\")\n",
        "    if scaling :\n",
        "        scaler = StandardScaler().fit(train)\n",
        "        s=train[sens]\n",
        "        st=test[sens]\n",
        "        t=train[y]\n",
        "        tt=test[y]\n",
        "        scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
        "        train = train.pipe(scale_df, scaler)\n",
        "        test = test.pipe(scale_df, scaler)\n",
        "        train= train.drop(columns_delete, axis=1)\n",
        "        train[sens] = s\n",
        "        train[y] = t\n",
        "        test= test.drop(columns_delete, axis=1)\n",
        "        test[sens] = st\n",
        "        test[y] = tt\n",
        "\n",
        "    return train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc9BzZv7FFHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "\n",
        "    \n",
        "    import numpy as np\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.utils import shuffle\n",
        "    from torch.autograd import Variable\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.disable_v2_behavior() \n",
        "    \n",
        "    \n",
        "    def p_rule(y_pred, z_values, threshold=0.5):\n",
        "        y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
        "        y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
        "        odds = y_z_1.mean() / y_z_0.mean()\n",
        "        return np.min([odds, 1/odds]) * 100\n",
        "    \n",
        "    def lossgr(y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "    \n",
        "    \n",
        "    class FAGTB(object):\n",
        "    \n",
        "        def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
        "                     min_impurity, max_depth, max_features, regression):\n",
        "            self.n_estimators = n_estimators\n",
        "            self.learning_rate = learning_rate\n",
        "            self.min_samples_split = min_samples_split\n",
        "            self.min_impurity = min_impurity\n",
        "            self.max_depth = max_depth\n",
        "            self.regression = regression\n",
        "            self.max_features = max_features\n",
        "    \n",
        "            # Initialize regression trees\n",
        "            self.trees = []\n",
        "            self.clfs = []\n",
        "            self.lossfunction_adv =[]\n",
        "            self.losstraining =[]\n",
        "    \n",
        "            for _ in range(n_estimators):\n",
        "                tree = DecisionTreeRegressor(criterion='friedman_mse', max_depth=9,\n",
        "      max_features=self.max_features, max_leaf_nodes=None,\n",
        "      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "      min_samples_leaf=1, min_samples_split=2,\n",
        "      min_weight_fraction_leaf=0.0\n",
        "      , random_state=0)\n",
        "                self.trees.append(tree)\n",
        "                clf = LogisticRegression()           \n",
        "                self.clfs.append(clf)\n",
        "                self.model = []\n",
        "        def fit2(self, X, y, sensitive, LAMBDA):\n",
        "            clf = LogisticRegression()\n",
        "            clf._initialize_parameters(sensitive)\n",
        "            print(clf.param)\n",
        "    \n",
        "        def gradient(self, y, p):\n",
        "            # Avoid division by zero\n",
        "            p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "            return - (y / p) + (1 - y) / (1 - p)\n",
        "    \n",
        "        def fit(self, X, y, sensitive, LAMBDA, Xtest, yt, sensitivet, adv):\n",
        "    \n",
        "            y2 = np.expand_dims(sensitive, axis=1)\n",
        "    \n",
        "            #for i in range(200):\n",
        "            #    clf.fit(y,sensitive,1)\n",
        "            lfadv =0\n",
        "    \n",
        "            self.Init = np.log(np.sum(y)/np.sum(1-y))\n",
        "            #self.Init = np.mean(y)\n",
        "            y_pred2 = np.full(np.shape(y), self.Init)\n",
        "            y_pred = np.full(np.shape(y), self.Init)\n",
        "            y_predt = np.full(np.shape(yt), self.Init)\n",
        "            t =np.full(np.shape(y), 0)\n",
        "            t2 =np.full(np.shape(yt), 0)\n",
        "    #        self.clfs.append(clf)\n",
        "            self.LAMBDA = LAMBDA\n",
        "            proj = 0\n",
        "            table = [0,0,0,0]\n",
        "            y_pred2 = np.expand_dims(1/(1+np.exp(-y_pred)), axis=1)\n",
        "    \n",
        "            graph = tf.Graph()\n",
        "            seed = 7 # for reproducible purpose\n",
        "            input_size =  1 # number of features\n",
        "            #epochs = 2# I've tested previously that this is the best epochs to avoid overfitting\n",
        "            learning_rate2 = 0.01\n",
        "            with graph.as_default():\n",
        "                #tf.set_random_seed(seed)\n",
        "                #np.random.seed(seed)\n",
        "            \n",
        "                X_input = tf.placeholder(dtype=tf.float32, shape=[None, input_size], name='X_input')\n",
        "                y_input = tf.placeholder(dtype=tf.float32, shape=[None, 1], name='y_input')\n",
        "                \n",
        "                W1 = tf.Variable(tf.random_normal(shape=[input_size, 1], seed=seed), name='W1', trainable=True)\n",
        "                b1 = tf.Variable(tf.random_normal(shape=[1], seed=seed), name='b1', trainable=True)\n",
        "                sigm = tf.nn.sigmoid(tf.add(tf.matmul(X_input, W1), b1), name='pred')\n",
        "                logit = tf.add(tf.matmul(X_input, W1), b1)\n",
        "                loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_input,\n",
        "                                                                        logits=logit, name='loss'))\n",
        "                train_steps = tf.train.GradientDescentOptimizer(learning_rate2).minimize(loss)\n",
        "            \n",
        "                sigm2 = tf.cast(sigm, tf.float32, name='sigm2') # 1 if >= 0.5\n",
        "                pred = tf.cast(tf.greater_equal(sigm, 0.5), tf.float32, name='pred') # 1 if >= 0.5\n",
        "                acc = tf.reduce_mean(tf.cast(tf.equal(pred, y_input), tf.float32), name='acc')\n",
        "                \n",
        "                init_var = tf.global_variables_initializer()\n",
        "                var_grad = tf.gradients(loss, X_input)[0]\n",
        "                \n",
        "            train_feed_dict = {X_input: y_pred2, y_input: y2}\n",
        "    \n",
        "            sess = tf.Session(graph=graph)\n",
        "            sess.run(init_var)\n",
        "            \n",
        "    \n",
        "            for i in range(self.n_estimators):\n",
        "\n",
        "                #if i ==0 :\n",
        "                #    for k in range(500):\n",
        "                #        clf.forward(y_pred2,y2)\n",
        "                #        clf.backward()\n",
        "    \n",
        "                y_pred2 = np.expand_dims(1/(1+np.exp(-y_pred)), axis=1)\n",
        "                #print(y_pred2)\n",
        "                train_feed_dict = {X_input: y_pred2, y_input: y2}   \n",
        "                sess.run(train_steps, feed_dict=train_feed_dict)\n",
        "                cur_loss = sess.run(loss, feed_dict=train_feed_dict)\n",
        "                train_acc = sess.run(acc, feed_dict=train_feed_dict)\n",
        "                S_ADV = sess.run(sigm2, feed_dict=train_feed_dict)\n",
        "                #print(train_acc)\n",
        "                #print(sess.run(W1))\n",
        "                #print('step {2}: loss {0:.5f}, train_acc {1:.2f}%'.format(\n",
        "                #                   cur_loss, 100*train_acc, i))          \n",
        "    \n",
        "                gradient_adv = sess.run(var_grad, feed_dict=train_feed_dict)\n",
        "                                \n",
        "                if abs(np.sum(gradient_adv)) <0.001 :\n",
        "                     print('erreur de gradient')\n",
        "                    # print(gradient_adv)           \n",
        "                #clf.forward(y_pred2,y2)\n",
        "                #print(clf.gradient_adv())\n",
        "                #print(clf.gradient_adv())\n",
        "                lfadv = gradient_adv*y_pred2*(1-y_pred2)    # *len(gradient_adv)       \n",
        "    \n",
        "                #clf.forward(y_pred2,y2)\n",
        "                #print(clf.predict(y_pred2))\n",
        "                #print(clf.gradient_adv())\n",
        "    \n",
        "\n",
        "                t=-np.squeeze(lfadv.T)\n",
        "                #print(t)\n",
        "                proj = 0\n",
        "                gradient = y- 1/(1+np.exp(-y_pred))- LAMBDA*t -proj\n",
        "                self.trees[i].fit(X, gradient)\n",
        "                update = self.trees[i].predict(X)\n",
        "                # Update y prediction\n",
        "                    #proj = 0            \n",
        "                y_pred += np.multiply(self.learning_rate, update)\n",
        "                y_fin = 1/(1+np.exp(-y_pred))\n",
        "    \n",
        "                losstraining = lossgr(y,y_fin)\n",
        "                lossglobal = losstraining - LAMBDA*t\n",
        "                #loss_adv = clf.lossfunction_adv(y_pred2,sensitive)\n",
        "                #test = clf.predict(y_pred2)\n",
        "                #tt = p_rule(clf.predict(y_pred)>0.5, np.squeeze(sensitive))\n",
        "                #print(f\"\\tgiven attribute sex;  {p_rule(clf.predict(y_pred)>0.5, np.squeeze(sensitive)):.0f}%-rule\")\n",
        "    \n",
        "                updatet = self.trees[i].predict(Xtest)\n",
        "                y_predt += np.multiply(self.learning_rate, updatet) \n",
        "                y_predt2=1/(1+np.exp(-y_predt))\n",
        "                accuracy = accuracy_score(y, np.squeeze(y_fin)>0.5)\n",
        "                accuracyt = accuracy_score(yt, np.squeeze(y_predt2)>0.5)\n",
        "                #if np.sum(clf.gradient_adv()+(sensitive-clf.predict(y_pred2).T)*p.data.numpy()) >1 :\n",
        "                #    print('erreur de gradient par pytorch')\n",
        "    \n",
        "    \n",
        "                if i % 5 == 0:\n",
        "                    print (i,np.sum(lfadv),np.sum(losstraining),np.sum(lossglobal), \"Accuracy:\", round(accuracy,4), \" test : \", round(accuracyt,4), \" Prule Train : \", p_rule(y_fin, sensitive)/100,\" Prule test : \", p_rule(y_predt2, sensitivet)/100)\n",
        "                table = np.vstack([table,[accuracy,accuracyt, p_rule(y_fin, sensitive)/100, p_rule(y_predt2, sensitivet)/100]])\n",
        "            return {'y_pred2':y_pred2,'S_ADV':S_ADV}\n",
        "    \n",
        "        def predict(self, X):\n",
        "            y_pred = np.full(np.shape(X)[0],self.Init, self.Init)\n",
        "    \n",
        "            for i in range(self.n_estimators):\n",
        "                update = self.trees[i].predict(X)\n",
        "                y_pred += np.multiply(self.learning_rate, update)\n",
        "                y_fin = 1/(1+np.exp(-y_pred))\n",
        "            # Set label to the value that maximizes probability\n",
        "            return y_fin\n",
        "    \n",
        "    \n",
        "    class GradientBoostingRegressor_AXA(FAGTB):\n",
        "        def __init__(self, n_estimators=200, learning_rate=0.05, min_samples_split=2,\n",
        "                     min_var_red=1e-7, max_depth=4, max_features = 7, debug=False):\n",
        "            super(GradientBoostingRegressor_AXA, self).__init__(n_estimators=n_estimators,\n",
        "                learning_rate=learning_rate,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_impurity=min_var_red,\n",
        "                max_depth=max_depth,\n",
        "                regression=True,\n",
        "                max_features = max_features)\n",
        "    \n",
        "    class GradientBoostingClassifier_AXA(FAGTB):\n",
        "        def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
        "                     min_info_gain=1e-7, max_depth=2, max_features = 7, debug=False):\n",
        "            super(GradientBoostingClassifier_AXA, self).__init__(n_estimators=n_estimators,\n",
        "                learning_rate=learning_rate,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_impurity=min_info_gain,\n",
        "                max_depth=max_depth,\n",
        "                regression=False,\n",
        "                max_features = max_features)\n",
        "    \n",
        "    \n",
        "    \n",
        "    class Sigmoid():\n",
        "        def __call__(self, x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "        def gradient(self, x):\n",
        "            return self.__call__(x) * (1 - self.__call__(x))\n",
        "    \n",
        "    \n",
        "    class LogisticRegression():\n",
        "        def __init__(self, learning_rate=.1):\n",
        "            self.param = None\n",
        "            self.learning_rate = learning_rate\n",
        "            self.sigmoid = Sigmoid()\n",
        "    \n",
        "        def _initialize_parameters(self, X):\n",
        "            n_features = np.shape(X)[1]\n",
        "            limit = 1 / math.sqrt(n_features)\n",
        "            self.param = np.random.uniform(-limit, limit, (n_features,))\n",
        "    #        self.param = [0]\n",
        "    \n",
        "        def fit(self, X, y, iteration):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            self.param -= self.learning_rate * -(y - y_pred).dot(X)\n",
        "            return self.param\n",
        "    \n",
        "        def gradient_adv(self,X,y):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            gradient_adv = (y - y_pred)*self.param*X.T*(1-X).T\n",
        "            return gradient_adv\n",
        "    \n",
        "        def predict(self, X):\n",
        "            y_pred = np.round(self.sigmoid(X.dot(self.param))).astype(int)\n",
        "            return y_pred\n",
        "    \n",
        "        def lossfunction(self,X,y):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            return lossgr(y,y_pred)\n",
        "        \n",
        "        def lossfunction_adv(self,X,y):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            return y-y_pred\n",
        "    \n",
        "        def param2(self):\n",
        "            return 2*self.param\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    class Net(nn.Module):\n",
        "        \n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.fc1 = nn.Linear(1, 32)\n",
        "            self.fc2 = nn.Linear(32, 15)\n",
        "            self.fc3 = nn.Linear(15, 5)\n",
        "            self.fc4 = nn.Linear(5, 1)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            x = self.fc1(x)\n",
        "            x = self.fc2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.fc3(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.fc4(x)\n",
        "            x = F.torch.sigmoid(x)\n",
        "            return x\n",
        "    \n",
        "    \n",
        "    class Logistic(nn.Module):\n",
        "        \n",
        "        def __init__(self):\n",
        "            super(Logistic, self).__init__()\n",
        "            self.fc1 = nn.Linear(1, 1)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            x = self.fc1(x)\n",
        "            x = F.torch.sigmoid(x)\n",
        "            return x\n",
        "    \n",
        "    "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAlyPoMBXY9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPobtlKhXaLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5815b565-03c7-44cb-cf45-b16522801985"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24129, 97)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jruLj9ewE9LT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a25885f-4698-41a0-9c99-b3f18e4b68a1"
      },
      "source": [
        "\n",
        "import copy\n",
        "# 315 trees for PC FIXE\n",
        "table = [0,0,0,0]\n",
        "for i in range(5):\n",
        "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
        "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race','race'])\n",
        "    \n",
        "    classifier= FAGTB(n_estimators=260, learning_rate = 0.01, max_depth = 10,min_samples_split=1.0, min_impurity =False, max_features =20, regression =1)\n",
        "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.165, Xtest=X_test.values, yt=y_test,\n",
        "                      sensitivet=sensitivet,adv=Logistic())\n",
        "    \n",
        "    ##### Results on Test dataset #####\n",
        "    y_predt2 = classifier.predict(X_test.values)\n",
        "    print('')\n",
        "    print('Results on test set :')\n",
        "    #display_results(y_predt2, y_test.values, sensitivet)\n",
        "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
        "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
        "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 16636.50076498833 13522.418706250504 16267.441332473581 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 50196.97507164773 13444.127173565501 21726.628060387375 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10 11861.880784581703 13366.390049324906 15323.600378780888 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15 27951.01895594709 13310.465201607349 17922.38332933862 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20 48748.71696522686 13235.543353977137 21279.08165323957 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25 20900.898094179815 13156.187498776882 16604.83568431655 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30 24363.658034734657 13097.762331468584 17117.765907199806 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "35 22123.190648789125 13029.076110362163 16679.40256741237 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40 53943.48603113797 12970.702772613766 21871.37796775153 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45 6347.518538829006 12902.72915319742 13950.069712104207 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50 35623.94729387694 12827.441744383812 18705.393047873506 Accuracy: 0.7511  test :  0.7509  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ef9925b09141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFAGTB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_impurity\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.165, Xtest=X_test.values, yt=y_test,\n\u001b[0;32m---> 11\u001b[0;31m                       sensitivet=sensitivet,adv=Logistic())\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m##### Results on Test dataset #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-fb119b3cf216>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sensitive, LAMBDA, Xtest, yt, sensitivet, adv)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m#print(y_pred2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mtrain_feed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkV32GfFRB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9032eff8-9ce9-47d4-97e7-5ec61281359c"
      },
      "source": [
        "np.set_printoptions(suppress=True) \n",
        "np.mean(table[1:,], axis=0).astype(float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([84.09083375, 88.05913736,  0.03771334,  0.27332308])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5othDpTlKYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}