{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FAGTB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXfkKA4Xers2lZrtzar4r6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent-grari/FAGTB/blob/master/FAGTB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmKbXdtm_BvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "2a956b49-14e0-4c47-9d66-0f4c562ce292"
      },
      "source": [
        "!pip3 install fairness\n",
        "! git clone https://github.com/vincent-grari/FAGTB"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairness in /usr/local/lib/python3.6/dist-packages (0.1.8)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fairness) (2018.9)\n",
            "Requirement already satisfied: BlackBoxAuditing>=0.1.26ggplot in /usr/local/lib/python3.6/dist-packages (from fairness) (0.1.54)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from fairness) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from fairness) (0.22.2.post1)\n",
            "Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (0.34.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from fairness) (2.4.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from fairness) (1.4.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from fairness) (0.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (2.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.1->fairness) (0.15.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->fairness) (1.1.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (1.2.0)\n",
            "fatal: destination path 'FAGTB' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8t15SobVx31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d260ee67-84d7-4756-c968-ff0e99f7a092"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://grari.fr/Fairness/ProcessedData.py', '/usr/local/lib/python3.6/dist-packages/fairness/data/objects/ProcessedData.py')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/usr/local/lib/python3.6/dist-packages/fairness/data/objects/ProcessedData.py',\n",
              " <http.client.HTTPMessage at 0x7f12d2ea1cf8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNxgkEwlE0-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "eaff1df9-5024-4c6a-b2c5-34ee2a869c4a"
      },
      "source": [
        "import fire\n",
        "import os\n",
        "import statistics\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from fairness import results\n",
        "from fairness.data.objects.list import DATASETS, get_dataset_names\n",
        "from fairness.data.objects.ProcessedData import ProcessedData\n",
        "from fairness.benchmark import run_alg\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from fairness.algorithms.zafar.ZafarAlgorithm import ZafarAlgorithmBaseline, ZafarAlgorithmAccuracy, ZafarAlgorithmFairness\n",
        "from fairness.algorithms.kamishima.KamishimaAlgorithm import KamishimaAlgorithm\n",
        "from fairness.algorithms.kamishima.CaldersAlgorithm import CaldersAlgorithm\n",
        "from fairness.algorithms.feldman.FeldmanAlgorithm import FeldmanAlgorithm\n",
        "from fairness.algorithms.baseline.SVM import SVM\n",
        "from fairness.algorithms.baseline.DecisionTree import DecisionTree\n",
        "from fairness.algorithms.baseline.GaussianNB import GaussianNB\n",
        "from fairness.algorithms.baseline.LogisticRegression import LogisticRegression\n",
        "from fairness.algorithms.ParamGridSearch import ParamGridSearch\n",
        "from fairness.algorithms.Ben.SDBSVM import SDBSVM\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def p_rule(y_pred, z_values, threshold=0.5):\n",
        "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
        "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
        "    odds = y_z_1.mean() / y_z_0.mean()\n",
        "    return np.min([odds, 1/odds]) * 100\n",
        "\n",
        "\n",
        "class Sigmoid():\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    def gradient(self, x):\n",
        "        return self.__call__(x) * (1 - self.__call__(x))\n",
        "    \n",
        "def DispFNR(y_pred, y, z_values, threshold=0.5):\n",
        "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
        "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
        "    result=abs(ypred_z_1[(y==1) & (z_values==0)].mean()-ypred_z_1[(y==1) & (z_values==1)].mean())\n",
        "    return result\n",
        "def DispFPR(y_pred, y, z_values, threshold=0.5):\n",
        "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
        "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
        "    result=abs(ypred_z_1[(y==0) & (z_values==0)].mean()-ypred_z_1[(y==0) & (z_values==1)].mean())\n",
        "    return result\n",
        "\n",
        "\n",
        "def DI(y_pred, z_values, threshold=0.5):\n",
        "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
        "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
        "    odds = abs(y_z_1.mean() - y_z_0.mean())\n",
        "    return odds\n",
        "\n",
        "def display_results(y_pred, y, sensitive):\n",
        "    y_pred2 = (y_pred>0.5).astype(int)\n",
        "    accuracy = accuracy_score(y, np.squeeze(y_pred2))\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"PRULE : \", p_rule(y_pred,sensitive))\n",
        "    print(\"DI : \", DI(y_pred, sensitive))\n",
        "    print(\"DispFPR : \", DispFPR(y_pred2, y, sensitive))\n",
        "    print(\"DispFNR : \", DispFNR(y_pred2, y, sensitive))\n",
        "    return {'Accuracy': accuracy, 'PRULE': p_rule(y_pred,sensitive), 'DispFPR': DispFPR(y_pred2, y, sensitive)\n",
        "            ,'DispFNR': DispFNR(y_pred2, y, sensitive)}\n",
        "\n",
        "def DATA_TRAIN_TEST(num,sens,y,columns_delete):\n",
        "    dataset = DATASETS[num] # Adult data set\n",
        "    all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
        "    ProcessedData(dataset)\n",
        "    processed_dataset = ProcessedData(dataset)\n",
        "    train_test_splits = processed_dataset.create_train_test_splits(1)\n",
        "    train_test_splits.keys()\n",
        "    train, test = train_test_splits['numerical-binsensitive'][0]\n",
        "    X_train = train\n",
        "    X_test = test\n",
        "    sensitive =  train[sens].values\n",
        "    sensitivet =  test[sens].values\n",
        "    y_train = train[y]\n",
        "    y_test = test[y]\n",
        "    \n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    s=X_train[sens]\n",
        "    st=X_test[sens]\n",
        "    t=X_train[y]\n",
        "    tt=X_test[y]\n",
        "    scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
        "    X_train = X_train.pipe(scale_df, scaler)\n",
        "    X_test = X_test.pipe(scale_df, scaler)\n",
        "    X_train= X_train.drop([sens,y], axis=1)\n",
        "    X_train[sens] = s\n",
        "    X_train[y] = t\n",
        "    X_test= X_test.drop([sens,y], axis=1)\n",
        "    X_test[sens] = st\n",
        "    X_test[y] = tt\n",
        "\n",
        "    X_train = X_train.drop(columns_delete,1)\n",
        "    X_test = X_test.drop(columns_delete,1)\n",
        "    return X_train, X_test, y_train, y_test, sensitive, sensitivet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available algorithms:\n",
            "  SVM\n",
            "  GaussianNB\n",
            "  LR\n",
            "  DecisionTree\n",
            "  Kamishima\n",
            "  Calders\n",
            "  ZafarBaseline\n",
            "  ZafarFairness\n",
            "  ZafarAccuracy\n",
            "  Kamishima-accuracy\n",
            "  Kamishima-DIavgall\n",
            "  Feldman-SVM\n",
            "  Feldman-GaussianNB\n",
            "  Feldman-LR\n",
            "  Feldman-DecisionTree\n",
            "  Feldman-SVM-DIavgall\n",
            "  Feldman-SVM-accuracy\n",
            "  Feldman-GaussianNB-DIavgall\n",
            "  Feldman-GaussianNB-accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc9BzZv7FFHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a276c898-6689-44d0-d886-c5c9844cd012"
      },
      "source": [
        "    \n",
        "\n",
        "    \n",
        "    import numpy as np\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.utils import shuffle\n",
        "    from torch.autograd import Variable\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.disable_v2_behavior() \n",
        "    \n",
        "    \n",
        "    def p_rule(y_pred, z_values, threshold=0.5):\n",
        "        y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
        "        y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
        "        odds = y_z_1.mean() / y_z_0.mean()\n",
        "        return np.min([odds, 1/odds]) * 100\n",
        "    \n",
        "    def lossgr(y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "    \n",
        "    \n",
        "    class FAGTB(object):\n",
        "    \n",
        "        def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
        "                     min_impurity, max_depth, max_features, regression):\n",
        "            self.n_estimators = n_estimators\n",
        "            self.learning_rate = learning_rate\n",
        "            self.min_samples_split = min_samples_split\n",
        "            self.min_impurity = min_impurity\n",
        "            self.max_depth = max_depth\n",
        "            self.regression = regression\n",
        "            self.max_features = max_features\n",
        "    \n",
        "            # Initialize regression trees\n",
        "            self.trees = []\n",
        "            self.clfs = []\n",
        "            self.lossfunction_adv =[]\n",
        "            self.losstraining =[]\n",
        "    \n",
        "            for _ in range(n_estimators):\n",
        "                tree = DecisionTreeRegressor(criterion='friedman_mse', max_depth=9,\n",
        "      max_features=self.max_features, max_leaf_nodes=None,\n",
        "      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "      min_samples_leaf=1, min_samples_split=2,\n",
        "      min_weight_fraction_leaf=0.0\n",
        "      , random_state=0)\n",
        "                self.trees.append(tree)\n",
        "                clf = LogisticRegression()           \n",
        "                self.clfs.append(clf)\n",
        "                self.model = []\n",
        "        def fit2(self, X, y, sensitive, LAMBDA):\n",
        "            clf = LogisticRegression()\n",
        "            clf._initialize_parameters(sensitive)\n",
        "            print(clf.param)\n",
        "    \n",
        "        def gradient(self, y, p):\n",
        "            # Avoid division by zero\n",
        "            p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "            return - (y / p) + (1 - y) / (1 - p)\n",
        "    \n",
        "        def fit(self, X, y, sensitive, LAMBDA, Xtest, yt, sensitivet):\n",
        "    \n",
        "            y2 = np.expand_dims(sensitive, axis=1)\n",
        "    \n",
        "            lfadv =0\n",
        "    \n",
        "            self.Init = np.log(np.sum(y)/np.sum(1-y))\n",
        "\n",
        "            y_pred2 = np.full(np.shape(y), self.Init)\n",
        "            y_pred = np.full(np.shape(y), self.Init)\n",
        "            y_predt = np.full(np.shape(yt), self.Init)\n",
        "            t =np.full(np.shape(y), 0)\n",
        "            t2 =np.full(np.shape(yt), 0)\n",
        "            self.LAMBDA = LAMBDA\n",
        "            proj = 0\n",
        "            table = [0,0,0,0]\n",
        "            y_pred2 = np.expand_dims(1/(1+np.exp(-y_pred)), axis=1)\n",
        "    \n",
        "            graph = tf.Graph()\n",
        "            seed = 7 # for reproducible purpose\n",
        "            input_size =  1 # number of features\n",
        "\n",
        "            learning_rate2 = 0.01\n",
        "            with graph.as_default():\n",
        "\n",
        "                X_input = tf.placeholder(dtype=tf.float32, shape=[None, input_size], name='X_input')\n",
        "                y_input = tf.placeholder(dtype=tf.float32, shape=[None, 1], name='y_input')\n",
        "                \n",
        "                W1 = tf.Variable(tf.random_normal(shape=[input_size, 1], seed=seed), name='W1', trainable=True)\n",
        "                b1 = tf.Variable(tf.random_normal(shape=[1], seed=seed), name='b1', trainable=True)\n",
        "                sigm = tf.nn.sigmoid(tf.add(tf.matmul(X_input, W1), b1), name='pred')\n",
        "                logit = tf.add(tf.matmul(X_input, W1), b1)\n",
        "                loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_input,\n",
        "                                                                        logits=logit, name='loss'))\n",
        "                train_steps = tf.train.GradientDescentOptimizer(learning_rate2).minimize(loss)\n",
        "            \n",
        "                sigm2 = tf.cast(sigm, tf.float32, name='sigm2') # 1 if >= 0.5\n",
        "                pred = tf.cast(tf.greater_equal(sigm, 0.5), tf.float32, name='pred') # 1 if >= 0.5\n",
        "                acc = tf.reduce_mean(tf.cast(tf.equal(pred, y_input), tf.float32), name='acc')\n",
        "                \n",
        "                init_var = tf.global_variables_initializer()\n",
        "                var_grad = tf.gradients(loss, X_input)[0]\n",
        "                \n",
        "            train_feed_dict = {X_input: y_pred2, y_input: y2}\n",
        "    \n",
        "            sess = tf.Session(graph=graph)\n",
        "            sess.run(init_var)\n",
        "            \n",
        "    \n",
        "            for i in range(self.n_estimators):\n",
        "\n",
        "                y_pred2 = np.expand_dims(1/(1+np.exp(-y_pred)), axis=1)\n",
        "\n",
        "                train_feed_dict = {X_input: y_pred2, y_input: y2}   \n",
        "                sess.run(train_steps, feed_dict=train_feed_dict)\n",
        "                cur_loss = sess.run(loss, feed_dict=train_feed_dict)\n",
        "                train_acc = sess.run(acc, feed_dict=train_feed_dict)\n",
        "                S_ADV = sess.run(sigm2, feed_dict=train_feed_dict)\n",
        "\n",
        "                gradient_adv = sess.run(var_grad, feed_dict=train_feed_dict)\n",
        "                                \n",
        "                if abs(np.sum(gradient_adv)) <0.001 :\n",
        "                     print('erreur de gradient')\n",
        "\n",
        "                lfadv = gradient_adv*y_pred2*(1-y_pred2)    # *len(gradient_adv)       \n",
        "\n",
        "                t=-np.squeeze(lfadv.T)\n",
        "                proj = 0\n",
        "                gradient = y- 1/(1+np.exp(-y_pred))- LAMBDA*t -proj\n",
        "                self.trees[i].fit(X, gradient)\n",
        "                update = self.trees[i].predict(X)\n",
        " \n",
        "                y_pred += np.multiply(self.learning_rate, update)\n",
        "                y_fin = 1/(1+np.exp(-y_pred))\n",
        "    \n",
        "                losstraining = lossgr(y,y_fin)\n",
        "                lossglobal = losstraining - LAMBDA*t\n",
        "\n",
        "                updatet = self.trees[i].predict(Xtest)\n",
        "                y_predt += np.multiply(self.learning_rate, updatet) \n",
        "                y_predt2=1/(1+np.exp(-y_predt))\n",
        "                accuracy = accuracy_score(y, np.squeeze(y_fin)>0.5)\n",
        "                accuracyt = accuracy_score(yt, np.squeeze(y_predt2)>0.5)\n",
        "    \n",
        "                if i % 5 == 0:\n",
        "                    print (i,np.sum(lfadv),np.sum(losstraining),np.sum(lossglobal), \"Accuracy:\", round(accuracy,4), \" test : \", round(accuracyt,4), \" Prule Train : \", p_rule(y_fin, sensitive)/100,\" Prule test : \", p_rule(y_predt2, sensitivet)/100)\n",
        "                table = np.vstack([table,[accuracy,accuracyt, p_rule(y_fin, sensitive)/100, p_rule(y_predt2, sensitivet)/100]])\n",
        "            return {'y_pred2':y_pred2,'S_ADV':S_ADV}\n",
        "    \n",
        "        def predict(self, X):\n",
        "            y_pred = np.full(np.shape(X)[0],self.Init, self.Init)\n",
        "    \n",
        "            for i in range(self.n_estimators):\n",
        "                update = self.trees[i].predict(X)\n",
        "                y_pred += np.multiply(self.learning_rate, update)\n",
        "                y_fin = 1/(1+np.exp(-y_pred))\n",
        "            # Set label to the value that maximizes probability\n",
        "            return y_fin\n",
        "    \n",
        "    \n",
        "    class GradientBoostingRegressor_AXA(FAGTB):\n",
        "        def __init__(self, n_estimators=200, learning_rate=0.05, min_samples_split=2,\n",
        "                     min_var_red=1e-7, max_depth=4, max_features = 7, debug=False):\n",
        "            super(GradientBoostingRegressor_AXA, self).__init__(n_estimators=n_estimators,\n",
        "                learning_rate=learning_rate,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_impurity=min_var_red,\n",
        "                max_depth=max_depth,\n",
        "                regression=True,\n",
        "                max_features = max_features)\n",
        "    \n",
        "    class GradientBoostingClassifier_AXA(FAGTB):\n",
        "        def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
        "                     min_info_gain=1e-7, max_depth=2, max_features = 7, debug=False):\n",
        "            super(GradientBoostingClassifier_AXA, self).__init__(n_estimators=n_estimators,\n",
        "                learning_rate=learning_rate,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_impurity=min_info_gain,\n",
        "                max_depth=max_depth,\n",
        "                regression=False,\n",
        "                max_features = max_features)\n",
        "    \n",
        "    \n",
        "    \n",
        "    class Sigmoid():\n",
        "        def __call__(self, x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "        def gradient(self, x):\n",
        "            return self.__call__(x) * (1 - self.__call__(x))\n",
        "    \n",
        "    \n",
        "    class LogisticRegression():\n",
        "        def __init__(self, learning_rate=.1):\n",
        "            self.param = None\n",
        "            self.learning_rate = learning_rate\n",
        "            self.sigmoid = Sigmoid()\n",
        "    \n",
        "        def _initialize_parameters(self, X):\n",
        "            n_features = np.shape(X)[1]\n",
        "            limit = 1 / math.sqrt(n_features)\n",
        "            self.param = np.random.uniform(-limit, limit, (n_features,))\n",
        "    #        self.param = [0]\n",
        "    \n",
        "        def fit(self, X, y, iteration):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            self.param -= self.learning_rate * -(y - y_pred).dot(X)\n",
        "            return self.param\n",
        "    \n",
        "        def gradient_adv(self,X,y):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            gradient_adv = (y - y_pred)*self.param*X.T*(1-X).T\n",
        "            return gradient_adv\n",
        "    \n",
        "        def predict(self, X):\n",
        "            y_pred = np.round(self.sigmoid(X.dot(self.param))).astype(int)\n",
        "            return y_pred\n",
        "    \n",
        "        def lossfunction(self,X,y):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            return lossgr(y,y_pred)\n",
        "        \n",
        "        def lossfunction_adv(self,X,y):\n",
        "            y_pred = self.sigmoid(X.dot(self.param))\n",
        "            return y-y_pred\n",
        "    \n",
        "        def param2(self):\n",
        "            return 2*self.param\n",
        "        "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAlyPoMBXY9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPobtlKhXaLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b7d7b89-80aa-48ff-981f-6e7b2c5ee147"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24129, 97)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jruLj9ewE9LT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50214440-613f-493b-fe29-4c417f9c5add"
      },
      "source": [
        "\n",
        "import copy\n",
        "# 315 trees for PC FIXE\n",
        "table = [0,0,0,0]\n",
        "for i in range(5):\n",
        "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
        "   \n",
        "    classifier= FAGTB(n_estimators=260, learning_rate = 0.01, max_depth = 10,min_samples_split=1.0, min_impurity =False, max_features =20, regression =1)\n",
        "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.165, Xtest=X_test.values, yt=y_test,\n",
        "                      sensitivet=sensitivet)\n",
        "    \n",
        "    ##### Results on Test dataset #####\n",
        "    y_predt2 = classifier.predict(X_test.values)\n",
        "    print('')\n",
        "    print('Results on test set :')\n",
        "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
        "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
        "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 16721.32909404397 13549.350261936732 16308.369562453985 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 50264.10032190739 13472.249417127854 21765.82597024257 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10 13335.263949979675 13393.624050623235 15593.94260236988 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15 27281.1284505816 13330.57947200404 17831.965666350006 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20 31970.542401558778 13260.572783663032 18535.712279920233 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25 56664.19927540269 13168.708502412985 22518.301382854428 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30 7803.633412232054 13093.431609865425 14381.031122883716 Accuracy: 0.7501  test :  0.7548  Prule Train :  nan  Prule test :  nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-10c47888efbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFAGTB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_impurity\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.165, Xtest=X_test.values, yt=y_test,\n\u001b[0;32m---> 11\u001b[0;31m                       sensitivet=sensitivet)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m##### Results on Test dataset #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-650e2ac28ed2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sensitive, LAMBDA, Xtest, yt, sensitivet)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mLAMBDA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[1;32m    382\u001b[0m                                 X.indptr.dtype != np.intc):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \"\"\"\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2229\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkV32GfFRB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9032eff8-9ce9-47d4-97e7-5ec61281359c"
      },
      "source": [
        "np.set_printoptions(suppress=True) \n",
        "np.mean(table[1:,], axis=0).astype(float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([84.09083375, 88.05913736,  0.03771334,  0.27332308])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5othDpTlKYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}